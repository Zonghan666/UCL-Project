{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T16:27:32.811174Z",
     "start_time": "2018-04-01T16:27:31.323411Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gensim\n",
    "import math\n",
    "import collections \n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.tokenize import casual_tokenize, word_tokenize, sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "import seaborn as sns\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix as CFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:19.370027Z",
     "start_time": "2018-03-31T20:42:19.360203Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def body_stance_join(body, stance):\n",
    "    data = pd.merge(body, stance, on='Body ID')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T21:34:59.583449Z",
     "start_time": "2018-03-31T21:34:59.536923Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#split the merged training data to training subset and validation set\n",
    "def train_val_split(data, val_ratio=0.1, seed=3693):\n",
    "    \n",
    "    #get the list of stances label\n",
    "    labels = list(data.Stance.value_counts().to_dict().keys())\n",
    "    \n",
    "    #get training subset and validation subset with different Stance label\n",
    "    train = []\n",
    "    val = []\n",
    "    for label in labels:\n",
    "        index = data[data['Stance'] == label].index.tolist()\n",
    "        random.seed(3693)\n",
    "        random.shuffle(index)\n",
    "        val_num = len(index) // 10\n",
    "        val_index = index[0:val_num]\n",
    "        train_index = index[val_num:]\n",
    "    \n",
    "        validation_set = data.iloc[val_index]\n",
    "        training_set = data.iloc[train_index]\n",
    "\n",
    "        train.append(training_set)\n",
    "        val.append(validation_set)  \n",
    "    \n",
    "    #concat the four subsets of training set and validation set\n",
    "    train_set = pd.concat(train, axis=0)\n",
    "    val_set = pd.concat(val, axis=0)\n",
    "    \n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:19.406981Z",
     "start_time": "2018-03-31T20:42:19.401853Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#funstion to merge the train and test set to better preprocess\n",
    "def merge(train, test):\n",
    "    data = [train, test]\n",
    "    data = pd.concat(data, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:19.797936Z",
     "start_time": "2018-03-31T20:42:19.789640Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to split the train and test set\n",
    "def train_test_split(merged, train, test, dtype='body'):\n",
    "    m_train = train.shape[0]\n",
    "    train_set = merged[:m_train]\n",
    "    test_set = merged[m_train:]\n",
    "    \n",
    "    if dtype == 'stance':\n",
    "        test_set = test_set.drop(['Stance'], axis=1)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:20.274192Z",
     "start_time": "2018-03-31T20:42:20.267015Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to lower the bodies and the headlines\n",
    "def lower(body_data, stance_data):\n",
    "    body_data.articleBody = body_data.articleBody.str.lower()\n",
    "    stance_data.Headline = stance_data.Headline.str.lower()\n",
    "    \n",
    "    return body_data, stance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:20.513503Z",
     "start_time": "2018-03-31T20:42:20.508285Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    token = re.compile(\"[\\w]+(?=n't)|n't|'s|\\'m|\\'ll|[\\w]+|[.?!;,:]\")\n",
    "    tokens = token.findall(input)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:20.711928Z",
     "start_time": "2018-03-31T20:42:20.677875Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tokenize the bodies and headlines and then remove the stop words\n",
    "def tokenization_and_remove(body_data, stance_data):\n",
    "    sents_body = body_data.articleBody.tolist()\n",
    "    sents_headline = stance_data.Headline.tolist()\n",
    "    \n",
    "    result_body = []\n",
    "    result_headline = []\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    \n",
    "    for sent in sents_body:\n",
    "        token_list = [w for w in tokenize(sent) if w not in stop_word]\n",
    "        result_body.append(token_list)\n",
    "        \n",
    "    for sent in sents_headline:\n",
    "        token_list = [w for w in tokenize(sent) if w not in stop_word]\n",
    "        result_headline.append(token_list)\n",
    "        \n",
    "    body_data.articleBody = result_body\n",
    "    stance_data.Headline = result_headline\n",
    "    \n",
    "    return body_data, stance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:20.921695Z",
     "start_time": "2018-03-31T20:42:20.879916Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stemming \n",
    "def lemmatization(body_data, stance_data):\n",
    "    result_body = []\n",
    "    result_headline = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for word_list in body_data.articleBody:\n",
    "        sents = []\n",
    "        for word in word_list:\n",
    "            word_stem = WordNetLemmatizer().lemmatize(word, pos='v')\n",
    "            #word_stem = stemmer.stem(word)\n",
    "            sents.append(word_stem)\n",
    "        result_body.append(sents)\n",
    "        \n",
    "    for word_list in stance_data.Headline:\n",
    "        sents = []\n",
    "        for word in word_list:\n",
    "            word_stem = WordNetLemmatizer().lemmatize(word, pos='v')\n",
    "            #word_stem = stemmer.stem(word)\n",
    "            sents.append(word_stem)\n",
    "        result_headline.append(sents)\n",
    "        \n",
    "    body_data.articleBody = result_body\n",
    "    stance_data.Headline = result_headline\n",
    "    \n",
    "    return body_data, stance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:21.006523Z",
     "start_time": "2018-03-31T20:42:20.995920Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#combine all the process\n",
    "def preprocess(train_body, test_body, train_stance, test_stance, lemma=True):\n",
    "    merged_bodies = merge(train_bodies, test_bodies)\n",
    "    merged_stances = merge(train_stances, test_stances)\n",
    "    merged_bodies, merged_stances = lower(merged_bodies, merged_stances)\n",
    "    merged_bodies, merged_stances = tokenization_and_remove(merged_bodies, merged_stances)\n",
    "    if lemma == True:  \n",
    "        merged_bodies, merged_stances = lemmatization(merged_bodies, merged_stances)\n",
    "    \n",
    "    return merged_bodies, merged_stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:21.196236Z",
     "start_time": "2018-03-31T20:42:21.167728Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get the word corpus of the body and headline\n",
    "def get_corpus(body_data, stance_data):\n",
    "    corpus_body = []\n",
    "    corpus_headline = []\n",
    "    \n",
    "    for word_list in body_data.articleBody:\n",
    "        corpus_body += word_list\n",
    "        \n",
    "    for word_list in stance_data.Headline:\n",
    "        corpus_headline += word_list\n",
    "        \n",
    "    corpus = set(corpus_body + corpus_headline)\n",
    "    #add OOV to deal with unseen word in test set\n",
    "    corpus.add('OOV')\n",
    "    \n",
    "    word = corpus\n",
    "    index = range(len(word))\n",
    "    \n",
    "    word_to_index_dict = dict(zip(word, index))\n",
    "    index_to_word_dict = dict(zip(index, word))\n",
    "    corpus = list(corpus)\n",
    "    \n",
    "    return corpus, word_to_index_dict, index_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:21.444238Z",
     "start_time": "2018-03-31T20:42:21.434199Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#transfor data (string->integer or interger->string) according to the corpus dict\n",
    "def transform(corpus, data):\n",
    "    result = []\n",
    "    for word_list in data:\n",
    "        num_list = []\n",
    "        for word in word_list:\n",
    "            if word not in corpus:\n",
    "                #use oov token to represent the unseen word in test set\n",
    "                num_list.append(corpus['OOV'])\n",
    "            num_list.append(corpus[word])\n",
    "        result.append(num_list)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:21.819137Z",
     "start_time": "2018-03-31T20:42:21.717666Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def body_headline_match(vector_body, vector_headline, bodies, stances, distance='cos'):\n",
    "    #construct Dataframe for body and headline to merge the data on Body ID\n",
    "    body = pd.DataFrame(list(range(vector_body.shape[0])), columns=['body_idx'])\n",
    "    body['Body ID'] = bodies['Body ID']\n",
    "    headline = pd.DataFrame(list(range(vector_headline.shape[0])), columns=['headline_idx'])\n",
    "    headline['Body ID'] = stances['Body ID']\n",
    "    headline['Stance'] = stances['Stance']\n",
    "\n",
    "    #merge the body and the headline\n",
    "    data = body_stance_join(body, headline)\n",
    "\n",
    "    #get body index and headline index from the vector matrix\n",
    "    body_idx = data['body_idx'].tolist()\n",
    "    headline_idx = data['headline_idx'].tolist()\n",
    "\n",
    "    #get the vector\n",
    "    x_body = vector_body[body_idx]\n",
    "    x_headline = vector_headline[headline_idx]\n",
    "\n",
    "    #initial the matrix \n",
    "    distance_matrix = np.zeros(x_body.shape[0])\n",
    "    \n",
    "    if distance == 'cos':\n",
    "        #compute the cosine similarity\n",
    "        for i in range(distance_matrix.shape[0]):\n",
    "            distance_matrix[i] = cosine_similarity(x_body[i], x_headline[i])\n",
    "\n",
    "        #final data that contain index of body and headline and also the pair-wise cosine similarity\n",
    "        data['distance_or_similarity'] = distance_matrix\n",
    "    elif distance == 'eu':\n",
    "        #compute the Euclidean distance\n",
    "        for i in range(distance_matrix.shape[0]):\n",
    "            distance_matrix[i] = eu_distance(x_body[i], x_headline[i])\n",
    "\n",
    "        #final data that contain index of body and headline and also the pair-wise cosine similarity\n",
    "        data['distance_or_similarity'] = distance_matrix\n",
    "    \n",
    "    elif distance == 'KL':\n",
    "        for i in range(distance_matrix.shape[0]):\n",
    "            distance_matrix[i] = KL_divergance(x_body[i], x_headline[i])\n",
    "        \n",
    "        #final data that contain index of body and headline and also the pair-wise KL divergance\n",
    "        data['distance_or_similarity'] = distance_matrix\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:22.127019Z",
     "start_time": "2018-03-31T20:42:22.080014Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#function to get tf_idf matrix and the idf matrix(for test set)\n",
    "#when count is False, we use the frequency as the value. Otherwise, we use the count as the value\n",
    "def get_tf_idf_matrix(corpus, data, count=False):\n",
    "    \n",
    "    m = len(data)\n",
    "    n = len(corpus)\n",
    "    \n",
    "    count_matrix = np.zeros([m, n])\n",
    "    freq_matrix = np.zeros([m, n])\n",
    "    \n",
    "    for i in range(m):\n",
    "        document = data.iloc[i]\n",
    "        count_dict = Counter(document)\n",
    "        index = list(count_dict.keys())\n",
    "        value = np.array(list(count_dict.values()))\n",
    "        count_matrix[i][index] = value\n",
    "        freq_matrix[i][index] = value / len(document)\n",
    "    \n",
    "    frequency =  (freq_matrix > 0).sum(axis=0)\n",
    "    idf = np.log10(np.divide(m, (1+frequency)))\n",
    "    idf = np.log10(m / (1 + frequency))\n",
    "    \n",
    "    tf_idf = np.multiply(freq_matrix, idf)\n",
    "\n",
    "    if count == True:  \n",
    "        return count_matrix, idf, tf_idf\n",
    "    elif count == False:\n",
    "        return freq_matrix, idf, tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## distance/similarity measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:22.723418Z",
     "start_time": "2018-03-31T20:42:22.713300Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2):\n",
    "    numerator = np.dot(x1.T, x2)\n",
    "    dominator = np.linalg.norm(x1) * np.linalg.norm(x2)\n",
    "    \n",
    "    cos_similarity = numerator / dominator\n",
    "    \n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:23.173119Z",
     "start_time": "2018-03-31T20:42:23.166899Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eu_distance(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:23.659982Z",
     "start_time": "2018-03-31T20:42:23.653937Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def KL_divergance(document_model, query_model):\n",
    "    KL = np.sum((query_model * np.log(query_model / document_model)))\n",
    "    \n",
    "    return KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:24.565169Z",
     "start_time": "2018-03-31T20:42:24.538758Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#function to get weighted(idf value)average word2vec based representation of document\n",
    "def weighted_average_word2vec(data, embeddings, lookup_dict, idf_matrix, vec_len=512):\n",
    "    \n",
    "    vec = np.zeros([data.shape[0],vec_len])\n",
    "\n",
    "    #word that not appear in embed or not english\n",
    "    OOV = voc[1]\n",
    "    \n",
    "    for idx,document in enumerate(data):\n",
    "        m = len(document)\n",
    "        \n",
    "        #element in the document are all numbers\n",
    "        for num in document:\n",
    "            #transform number to string according to num_to_word_dict\n",
    "            word = lookup_dict[num]\n",
    "            idf = idf_matrix[num]\n",
    "            \n",
    "            if word not in embed_dict:\n",
    "                vec[idx] += embeddings[OOV] * idf\n",
    "            else:\n",
    "                vec[idx] += embeddings[word] * idf\n",
    "    \n",
    "        vec[idx] /= m\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:25.032815Z",
     "start_time": "2018-03-31T20:42:25.015536Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#use tf-idf value to select the most n important word in a document\n",
    "def tf_idf_based_search(tf_idf_matrix, n=3):\n",
    "    m = tf_idf_matrix.shape[0]\n",
    "    \n",
    "    result = np.zeros([m,n])\n",
    "    \n",
    "    for i in range(m):\n",
    "        #get n index with the largest tf-idf value in descending order(the first one is the largest one)\n",
    "        sort_list = tf_idf_matrix[i].argsort()[-n:][::-1]\n",
    "        result[i] = sort_list\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:25.581742Z",
     "start_time": "2018-03-31T20:42:25.441975Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance(vector_body, vector_headline, bodies, stances, index_to_word_dict, n=3):\n",
    "    body = pd.DataFrame(list(range(vector_body.shape[0])), columns=['body_idx'])\n",
    "    body['Body ID'] = bodies['Body ID']\n",
    "    headline = pd.DataFrame(list(range(vector_headline.shape[0])), columns=['headline_idx'])\n",
    "    headline['Body ID'] = stances['Body ID']\n",
    "    headline['Stance'] = stances['Stance']\n",
    "\n",
    "    #merge the body and the headline\n",
    "    data = body_stance_join(body, headline)\n",
    "    \n",
    "    distance = np.zeros(data.shape[0])\n",
    "    \n",
    "    #for words that not exist in embed_dict, treat it as OOV token(non-english vector in the embedding)\n",
    "    OOV = voc[1]\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        \n",
    "        #initialse the accumlate_similarity \n",
    "        accumulate_distance = 0\n",
    "        \n",
    "        #get the index of words in a body and a headline\n",
    "        body_idx = data.body_idx.iloc[i]\n",
    "        headline_idx = data.headline_idx.iloc[i]\n",
    "        \n",
    "        #get the list of word(integer) of the specify body and headline\n",
    "        body_words = vector_body[body_idx]\n",
    "        headline_words = vector_headline[headline_idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #iterate n important words in the body\n",
    "        for num1 in body_words:\n",
    "            word1 = index_to_word_dict[num1]\n",
    "            if word1 not in embed_dict:\n",
    "                vec1 = embed_dict[OOV]\n",
    "            else:\n",
    "                vec1 = embed_dict[word1]\n",
    "            \n",
    "            record = np.zeros(n)\n",
    "            \n",
    "            #interate n important words in the headline\n",
    "            for index,num2 in enumerate(headline_words):\n",
    "                word2 = index_to_word_dict[num2]\n",
    "                if word2 not in embed_dict:\n",
    "                    vec2 = embed_dict[OOV]\n",
    "                else:\n",
    "                    vec2 = embed_dict[word2]  \n",
    "                \n",
    "                record[index] = eu_distance(vec1, vec2)\n",
    "            \n",
    "            # compute pair-wise cosine similarity, and select the largest one\n",
    "            accumulate_distance += record.min()\n",
    "        \n",
    "        #avarage\n",
    "        distance[i] = accumulate_distance\n",
    "        \n",
    "    data['distance_or_similarity'] = distance\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:26.459843Z",
     "start_time": "2018-03-31T20:42:26.442031Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_doc2vec_document(train_body, train_headline, test_body, test_headline):\n",
    "    #get the whole documents required for the genism doc2vec model\n",
    "    document = []\n",
    "\n",
    "    for i in range(train_body.shape[0]):\n",
    "        document.append(LabeledSentence(train_body.articleBody.iloc[i], ['train_body_' + str(i)]))\n",
    "\n",
    "    for i in range(train_stance.shape[0]):\n",
    "        document.append(LabeledSentence(train_stance.Headline.iloc[i], ['train_headline_' + str(i)]))\n",
    "\n",
    "    for i in range(test_body.shape[0]):\n",
    "            document.append(LabeledSentence(test_body.articleBody.iloc[i], ['test_body_' + str(i)]))\n",
    "\n",
    "    for i in range(test_stance.shape[0]):\n",
    "        document.append(LabeledSentence(test_stance.Headline.iloc[i], ['test_headline_' + str(i)]))\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:27.585151Z",
     "start_time": "2018-03-31T20:42:27.573906Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get background probability for words in corpus\n",
    "def get_background(data, tf_matrix):\n",
    "    \n",
    "    m_document = tf_matrix.shape[0]\n",
    "    background = np.zeros_like(tf_matrix)\n",
    "    \n",
    "    for i in range(m_document):\n",
    "        background[i] = tf_matrix[i]\n",
    "    \n",
    "    background = background.sum(axis=0)\n",
    "    \n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:28.148470Z",
     "start_time": "2018-03-31T20:42:28.124408Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_language_model(data, background, tf_matrix, smoothing='dirchelet', lambd=None):\n",
    "    \n",
    "    background_prob = background / background.sum()\n",
    "    m_document = len(data)\n",
    "    m_corpus = len(background)\n",
    "    \n",
    "    model = np.zeros([m_document, m_corpus])\n",
    "    \n",
    "    if smoothing == 'dirchelet':\n",
    "        u = background.sum() / len(data)\n",
    "        for i in range(m_document):\n",
    "            N = len(data.iloc[1])\n",
    "            m_word = tf_matrix[i].sum()\n",
    "            model[i] = N / (u + N) * tf_matrix[i] / m_word + u / (u + N) * background_prob\n",
    "    \n",
    "    if smoothing == 'jelinek':\n",
    "        for i in range(m_document):\n",
    "            m_word = tf_matrix[i].sum()\n",
    "            model[i] = lambd * tf_matrix[i] / m_word + (1-lambd) * background_prob\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load and merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:29.663716Z",
     "start_time": "2018-03-31T20:42:29.494192Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "train_bodies = pd.read_csv('fnc-1-master/train_bodies.csv')\n",
    "train_stances = pd.read_csv('fnc-1-master/train_stances.csv')\n",
    "\n",
    "test_bodies = pd.read_csv('fnc-1-master/competition_test_bodies.csv')\n",
    "test_stances = pd.read_csv('fnc-1-master/competition_test_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:42:30.601419Z",
     "start_time": "2018-03-31T20:42:30.581816Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = body_stance_join(train_bodies, train_stances)\n",
    "test_data = body_stance_join(test_bodies, test_stances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:45:05.607181Z",
     "start_time": "2018-03-31T20:42:31.651915Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#load the pre-trained word embedding (might takes few minutes)\n",
    "voc = [line.rstrip('\\n') for line in open('word2vec_embedding/embd_voc')]\n",
    "vec = np.loadtxt('word2vec_embedding/embd_vec')\n",
    "\n",
    "#create the embedding dictionary\n",
    "embed_dict = dict(zip(voc, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task1: Split the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:09:00.502480Z",
     "start_time": "2018-03-31T10:09:00.311461Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, valid = train_val_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:15:41.927473Z",
     "start_time": "2018-03-31T13:15:41.445886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot the bar to show ratio of four different classes in training set and validation set.\n",
    "plt.show()\n",
    "valid.Stance.value_counts().plot.bar()\n",
    "plt.show()\n",
    "train.Stance.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task2: Extract vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T21:21:05.399142Z",
     "start_time": "2018-03-27T21:21:05.395659Z"
    },
    "hidden": true
   },
   "source": [
    "## Bag-of-word and tf-idf based representation(Salton's vector space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:19:05.900210Z",
     "start_time": "2018-03-31T13:18:14.745248Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process, to reduce the dimension of\n",
    "#bag of word representation. I do the stemming to the text\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between index and word) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix, idf matrix and tf_idf matrix for body and headline\n",
    "tf_body, idf_body, tf_idf_body = get_tf_idf_matrix(corpus, train_body.articleBody, count=False)\n",
    "tf_headline, idf_headline, tf_idf_headline = get_tf_idf_matrix(corpus, train_stance.Headline, count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:21:39.202736Z",
     "start_time": "2018-03-31T13:20:45.542736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#match the body and headline according the Body ID and compute pair-wise cosine similarity\n",
    "data_method1 = body_headline_match(tf_idf_body, tf_idf_headline, train_bodies, train_stances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:21:39.707762Z",
     "start_time": "2018-03-31T13:21:39.210039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise cosine similarity distritbution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_method1[data_method1['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_method1[data_method1['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_method1[data_method1['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_method1[data_method1['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## word2vec and idf based representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:23:08.138193Z",
     "start_time": "2018-03-31T13:22:15.656425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "#for word2vec method, we don't have to do the stemming beacuse a words with different forms still have the likely vector representation\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix and idf matrix for body and headline\n",
    "tf_body, idf_body, tf_idf_body = get_tf_idf_matrix(corpus, train_body.articleBody, count=False)\n",
    "tf_headline, idf_headline, tf_idf_headline = get_tf_idf_matrix(corpus, train_stance.Headline, count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:23:16.780851Z",
     "start_time": "2018-03-31T13:23:08.153962Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vec_body = weighted_average_word2vec(train_body.articleBody, embed_dict, index_to_word_dict, idf_body)\n",
    "vec_headline = weighted_average_word2vec(train_stance.Headline, embed_dict, index_to_word_dict, idf_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:23:18.138816Z",
     "start_time": "2018-03-31T13:23:16.787559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#match the body and headline according the Body ID and compute pair-wise cosine similarity\n",
    "data_method2 = body_headline_match(vec_body, vec_headline, train_bodies, train_stances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:23:18.587088Z",
     "start_time": "2018-03-31T13:23:18.142473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise cosine similarity distritbution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_method2[data_method2['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_method2[data_method2['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_method2[data_method2['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "sns.kdeplot(data_method2[data_method2['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## word2vec and tf-idf based representation and word mover's distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:25:10.023135Z",
     "start_time": "2018-03-31T13:24:16.298015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "#for word2vec method, we don't have to do the stemming beacuse a words with different forms still have the likely vector representation\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix and idf matrix for body and headline\n",
    "tf_body, idf_body, tf_idf_body = get_tf_idf_matrix(corpus, train_body.articleBody, count=False)\n",
    "tf_headline, idf_headline, tf_idf_headline = get_tf_idf_matrix(corpus, train_stance.Headline, count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:25:42.992668Z",
     "start_time": "2018-03-31T13:25:10.039027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#extract n most important words from body and headline according to the tf-idf value\n",
    "important_words_body = tf_idf_based_search(tf_idf_body, n=7)\n",
    "important_words_headline = tf_idf_based_search(tf_idf_headline, n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:26:10.454875Z",
     "start_time": "2018-03-31T13:25:43.002586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_method3 = word_mover_distance(important_words_body, important_words_headline, train_bodies, train_stances, index_to_word_dict, n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:26:10.886505Z",
     "start_time": "2018-03-31T13:26:10.459619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise word mover's distance distribution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_method3[data_method3['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_method3[data_method3['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_method3[data_method3['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_method3[data_method3['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## doc2vec based representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:26:59.749580Z",
     "start_time": "2018-03-31T13:26:46.863562Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "#for word2vec method, we don't have to do the stemming beacuse a words with different forms still have the likely vector representation\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_num_dict, num_to_word_dict = get_corpus(train_body, train_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:29:30.607401Z",
     "start_time": "2018-03-31T13:26:59.751832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#get document of the doc2vec model\n",
    "document = get_doc2vec_document(train_body, train_stance, test_body, test_stance)\n",
    "\n",
    "#different parameters lead to different performance\n",
    "\n",
    "#define the doc2vecm model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=128, alpha=.025, min_alpha=.01, dm=0, worker=8, dbow_words=1)\n",
    "model.build_vocab(document)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(20):\n",
    "    model.train(document, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002  # decrease the learning rate`\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:29:31.606153Z",
     "start_time": "2018-03-31T13:29:30.611059Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#construct Dataframe for body and headline to merge the data on Body ID\n",
    "body = pd.DataFrame(list(range(train_body.shape[0])), columns=['body_idx'])\n",
    "body['Body ID'] = train_bodies['Body ID']\n",
    "headline = pd.DataFrame(list(range(train_stance.shape[0])), columns=['headline_idx'])\n",
    "headline['Body ID'] = train_stances['Body ID']\n",
    "headline['Stance'] = train_stances['Stance']\n",
    "\n",
    "#merge the body and the headline\n",
    "data_method4 = body_stance_join(body, headline)\n",
    "\n",
    "#get body index and headline index from the vector matrix\n",
    "body_idx = data_method4['body_idx'].tolist()\n",
    "headline_idx = data_method4['headline_idx'].tolist()\n",
    "\n",
    "similarity = np.zeros(data_method4.shape[0])\n",
    "\n",
    "for i in range(data_method4.shape[0]):\n",
    "    similarity[i] = model.docvecs.similarity(d1='train_body_'+str(body_idx[i]), d2='train_headline_'+str(headline_idx[i]))\n",
    "\n",
    "data_method4['distance_or_similarity'] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:29:31.969270Z",
     "start_time": "2018-03-31T13:29:31.609648Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise similarity distribution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_method4[data_method4['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_method4[data_method4['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_method4[data_method4['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_method4[data_method4['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task3: Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:35:40.480778Z",
     "start_time": "2018-03-31T13:35:03.546563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=False)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix for body and headline\n",
    "tf_body, _, _ = get_tf_idf_matrix(corpus, train_body.articleBody, count=True)\n",
    "tf_headline, _, _ = get_tf_idf_matrix(corpus, train_stance.Headline, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:36:37.628294Z",
     "start_time": "2018-03-31T13:35:40.500667Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#get the background of body and headline\n",
    "#As the corpus is obtained by combining the body and stance, in background_body, there are some wrods that not appear\n",
    "#in the body article and their probability will be zeros\n",
    "background_body = get_background(train_body['articleBody'], tf_body) + 1e-05\n",
    "background_headline = get_background(train_stance['Headline'], tf_headline) + 1e-05\n",
    "\n",
    "#get the language model of each individual body and headline\n",
    "language_model_body = get_language_model(train_body['articleBody'], background_body, tf_body, smoothing='dirchelet')\n",
    "language_model_headline = get_language_model(train_stance['Headline'], background_headline, tf_headline, smoothing='dirchelet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:39:46.159501Z",
     "start_time": "2018-03-31T13:36:37.654862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#get the pair-wise KL divergance(might takes about 5 minutes to compute)\n",
    "data_KL = body_headline_match(language_model_body, language_model_headline, train_bodies, train_stances, distance='KL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:40:04.577964Z",
     "start_time": "2018-03-31T13:40:03.970640Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise similarity distribution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_KL[data_KL['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_KL[data_KL['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_KL[data_KL['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_KL[data_KL['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task4ï¼šAlternative Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Topice Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:41:19.489462Z",
     "start_time": "2018-03-31T13:40:37.604085Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "#for word2vec method, we don't have to do the stemming beacuse a words with different forms still have the likely vector representation\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix for body and headline\n",
    "tf_body, _, tf_idf_body = get_tf_idf_matrix(corpus, train_body.articleBody, count=True)\n",
    "tf_headline, _, tf_idf_headline = get_tf_idf_matrix(corpus, train_stance.Headline, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:41:33.084797Z",
     "start_time": "2018-03-31T13:41:19.497508Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#concat body and headline data\n",
    "all_data = np.vstack([tf_body, tf_headline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:45:44.625172Z",
     "start_time": "2018-03-31T13:41:33.089138Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#implement svd\n",
    "svd = TruncatedSVD(n_components=25, n_iter=10, random_state=0)\n",
    "svd.fit(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:45:56.467548Z",
     "start_time": "2018-03-31T13:45:44.643569Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#transform the body and headline\n",
    "svd_body = svd.transform(tf_body)\n",
    "svd_headline = svd.transform(tf_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:45:57.418002Z",
     "start_time": "2018-03-31T13:45:56.478687Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_lda = body_headline_match(vector_body=svd_body, vector_headline=svd_headline, bodies=train_bodies, stances=train_stances, distance='cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:45:57.926361Z",
     "start_time": "2018-03-31T13:45:57.420727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise cosine similarity distritbution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:48:36.540089Z",
     "start_time": "2018-03-31T13:47:56.957508Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge the train and test set for body and stance data set to better pre-process\n",
    "#for word2vec method, we don't have to do the stemming beacuse a words with different forms still have the likely vector representation\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=False)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between string and integer) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(train_body, train_stance)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "#get tf matrix for body and headline\n",
    "tf_body, _, tf_idf_body = get_tf_idf_matrix(corpus, train_body.articleBody, count=True)\n",
    "tf_headline, _, tf_idf_headline = get_tf_idf_matrix(corpus, train_stance.Headline, count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:50:44.443591Z",
     "start_time": "2018-03-31T13:48:36.561896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#LDA model\n",
    "lda = LatentDirichletAllocation(n_topics=25, learning_method='batch', n_jobs=3, random_state=0)\n",
    "lda_body = lda.fit_transform(tf_body)\n",
    "lda_headline = lda.transform(tf_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:50:45.064480Z",
     "start_time": "2018-03-31T13:50:44.451225Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_lda = body_headline_match(vector_body=lda_body, vector_headline=lda_headline, bodies=train_bodies, stances=train_stances, distance='cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T13:50:45.407662Z",
     "start_time": "2018-03-31T13:50:45.068277Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pair-wise cosine similarity distritbution of different stances\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'unrelated'].distance_or_similarity, label='unrelated')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'discuss'].distance_or_similarity, label='discuss')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'agree'].distance_or_similarity, label='agree')\n",
    "sns.kdeplot(data_lda[data_lda['Stance'] == 'disagree'].distance_or_similarity, label='disagree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5: Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:45:05.762002Z",
     "start_time": "2018-03-31T20:45:05.612665Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, C=1, random_state=None, learning_rate=0.01, n_iteration=100, batch_size=100):\n",
    "        self.C = C\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iteration = n_iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batch = 0\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        seed = self.random_state\n",
    "        np.random.seed(seed)\n",
    "        self.w = np.random.random([n,1])\n",
    "        self.b = 0\n",
    "        \n",
    "        #construct the batch size\n",
    "        self.n_batch = math.ceil(m / self.batch_size)\n",
    "        batch = []\n",
    "        for i in range(self.n_batch - 1):\n",
    "            start = i * self.batch_size\n",
    "            end = (i + 1) * self.batch_size\n",
    "            batch.append(range(start, end))\n",
    "            \n",
    "        #last batch\n",
    "        start = (self.n_batch - 1) * self.batch_size\n",
    "        end = m\n",
    "        batch.append(range(start, end))\n",
    "        \n",
    "        for i in range(self.n_iteration):\n",
    "            current_loss = 0\n",
    "            for j in range(self.n_batch):\n",
    "                #get the batch\n",
    "                x_train = X[batch[j]]\n",
    "                y_train = y[batch[j]]\n",
    "                size = x_train.shape[0]\n",
    "                #forward\n",
    "                fx = np.dot(x_train, self.w) + self.b\n",
    "                #compute the loss\n",
    "                delta = fx - y_train\n",
    "                loss = 0.5 * np.dot(delta.T, delta) + 0.5 * self.C * np.sum(np.square(self.w))\n",
    "                #compute the gradient\n",
    "                dz = np.average(fx - y_train)\n",
    "                gradient_w = 1. / size *(np.dot(x_train.T, (fx - y_train))) + self.C * self.w\n",
    "                #gradient_w = np.average((x_train * (fx - y_train)), axis=0).reshape(-1,1) + self.C * self.w\n",
    "                gradient_b = dz\n",
    "                #update parameters\n",
    "                self.w -= self.learning_rate * gradient_w\n",
    "                self.b -= self.learning_rate * gradient_b\n",
    "                current_loss += loss\n",
    "            current_loss /= m\n",
    "            print('n_iteration:' + str(i))\n",
    "            print('current loss:' + str(current_loss))\n",
    "            print('------------')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        fx = np.dot(X, self.w) + self.b\n",
    "        return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T14:35:28.957403Z",
     "start_time": "2018-04-01T14:35:28.118314Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, C=1, random_state=None, learning_rate=0.01, n_iteration=100, batch_size=100, multi_class=False):\n",
    "        self.C = C\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iteration = n_iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.multi_class = multi_class\n",
    "        self.n_batch = 0\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        orig_shape = x.shape\n",
    "\n",
    "        if len(x.shape) > 1:\n",
    "            # Matrix\n",
    "            exp_minmax = lambda x: np.exp(x - np.max(x))\n",
    "            denom = lambda x: 1.0 / np.sum(x)\n",
    "            x = np.apply_along_axis(exp_minmax, 1, x)\n",
    "            denominator = np.apply_along_axis(denom, 1, x)\n",
    "\n",
    "            if len(denominator.shape) == 1:\n",
    "                denominator = denominator.reshape((denominator.shape[0], 1))\n",
    "\n",
    "            x = x * denominator\n",
    "        else:\n",
    "            # Vector\n",
    "            x_max = np.max(x)\n",
    "            x = x - x_max\n",
    "            numerator = np.exp(x)\n",
    "            denominator = 1.0 / np.sum(numerator)\n",
    "            x = numerator.dot(denominator)\n",
    "\n",
    "        assert x.shape == orig_shape\n",
    "        return x\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        #initialize the weight and bias\n",
    "        W = np.zeros([n, 1])\n",
    "        B = 0\n",
    "        \n",
    "        #construct the batch size\n",
    "        self.n_batch = math.ceil(m / self.batch_size)\n",
    "        batch = []\n",
    "        for i in range(self.n_batch - 1):\n",
    "            start = i * self.batch_size\n",
    "            end = (i + 1) * self.batch_size\n",
    "            batch.append(range(start, end))\n",
    "            \n",
    "        #last batch\n",
    "        start = (self.n_batch - 1) * self.batch_size\n",
    "        end = m\n",
    "        batch.append(range(start, end))\n",
    "            \n",
    "        for i in range(self.n_iteration):\n",
    "            current_loss = 0\n",
    "            for j in range(self.n_batch):\n",
    "                #get the batch\n",
    "                x_train = X[batch[j]]\n",
    "                y_train = y[batch[j]]\n",
    "                size = x_train.shape[0]\n",
    "                #forward\n",
    "                fx = np.dot(x_train, W) + B\n",
    "                z = self.sigmoid(fx)\n",
    "                #compute loss\n",
    "                loss = 0.5 * ((-y_train) * np.log(z) - (1-y_train) * np.log(1-z)).sum() + 0.5 * self.C * np.sum(np.square(W))\n",
    "                #compute the gradient\n",
    "                dz = np.average(z - y_train)\n",
    "                #gradient_W = 1. / size * (np.dot(x_train.T, (z - y_train))) + self.C * W\n",
    "                gradient_W = (np.average((x_train * (z - y_train)), axis=0)).reshape(-1,1) + self.C * W\n",
    "                gradient_B = dz\n",
    "                #update parameters\n",
    "                W -= self.learning_rate * gradient_W\n",
    "                B -= self.learning_rate * gradient_B\n",
    "                #update loss\n",
    "                current_loss += loss\n",
    "            current_loss /= m\n",
    "            print('n_iteration:' + str(i))\n",
    "            print('current loss:' + str(current_loss))\n",
    "            print('------------')\n",
    "            \n",
    "        return W, B\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        #reset the learning parameter\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        if self.multi_class == False:\n",
    "            y = y.reshape(-1,1)\n",
    "            W, B = self.train(X,y)\n",
    "            self.w.append(W)\n",
    "            self.b.append(B)\n",
    "        #for multiclass classification\n",
    "        elif self.multi_class == True:\n",
    "            n_class = y.shape[1]\n",
    "            #run one-vs-all algorithm and record the parameters of each classifier\n",
    "            for i in range(n_class):\n",
    "                W, B = self.train(X, y[:,i].reshape(-1,1))\n",
    "                self.w.append(W)\n",
    "                self.b.append(B)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.multi_class == False:\n",
    "            weight = self.w[0]\n",
    "            bias = self.b[0]\n",
    "            fx = np.dot(X, weight) + bias\n",
    "            hx = self.sigmoid(fx)\n",
    "            pred = hx\n",
    "        \n",
    "        #get prediction of different classifiers\n",
    "        elif self.multi_class == True:\n",
    "            n_class = len(self.w)\n",
    "            pred = []\n",
    "            for i in range(n_class):\n",
    "                weight = self.w[i]\n",
    "                bias = self.b[i]\n",
    "                fx = np.dot(X, weight) + bias\n",
    "                hx = self.sigmoid(fx)\n",
    "                pred.append(hx)\n",
    "            \n",
    "            #apply softmax to get the final prediction\n",
    "            pred = np.hstack(pred)\n",
    "            pred = self.softmax(pred)\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred_prob = self.predict_prob(X)\n",
    "        if self.multi_class == False:\n",
    "            pred = (pred_prob > 0.5) * 1.0\n",
    "        elif self.multi_class == True:\n",
    "            pred = np.zeros_like(pred_prob)\n",
    "            max_index = pred_prob.argmax(axis=1)\n",
    "            for i in range(X.shape[0]):\n",
    "                pred[i][max_index[i]] = 1\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.w, self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get final representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:55:51.505822Z",
     "start_time": "2018-03-31T20:45:06.483319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#might take 8-10 minu\n",
    "#-----------------preprocess-------------------------\n",
    "#merge the train and test set for body and stance data set to better pre-process, to reduce the dimension of\n",
    "#bag of word representation. I do the stemming to the text\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=True)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between index and word) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(merged_bodies, merged_stances)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "test_body['articleBody'] = transform(word_to_index_dict, test_body.articleBody)\n",
    "test_stance['Headline'] = transform(word_to_index_dict, test_stance.Headline)\n",
    "\n",
    "#get tf matrix, idf matrix and tf_idf matrix for body and headline\n",
    "tf_body_train, idf_body_train, tf_idf_body_train = get_tf_idf_matrix(corpus, train_body.articleBody, count=False)\n",
    "tf_headline_train, idf_headline_train, tf_idf_headline_train = get_tf_idf_matrix(corpus, train_stance.Headline, count=False)\n",
    "\n",
    "tf_body_test, idf_body_test, tf_idf_body_test = get_tf_idf_matrix(corpus, test_body.articleBody, count=False)\n",
    "tf_headline_test, idf_headline_test, tf_idf_headline_test = get_tf_idf_matrix(corpus, test_stance.Headline, count=False)\n",
    "\n",
    "print('preprocess finished')\n",
    "\n",
    "\n",
    "#-----------------bag-of-word-feature-------------------------\n",
    "data_method1_train = body_headline_match(tf_idf_body_train, tf_idf_headline_train, train_bodies, train_stances)\n",
    "data_method1_test = body_headline_match(tf_idf_body_test, tf_idf_headline_test, test_bodies, test_stances)\n",
    "\n",
    "print('bag of word feature finished')\n",
    "\n",
    "\n",
    "#-----------------word2vec-representation1-------------------------\n",
    "word2vec_1_body_train = weighted_average_word2vec(train_body.articleBody, embed_dict, index_to_word_dict, idf_body_train)\n",
    "word2vec_1_headline_train = weighted_average_word2vec(train_stance.Headline, embed_dict, index_to_word_dict, idf_headline_train)\n",
    "word2vec_1_body_test = weighted_average_word2vec(test_body.articleBody, embed_dict, index_to_word_dict, idf_body_test)\n",
    "word2vec_1_headline_test = weighted_average_word2vec(test_stance.Headline, embed_dict, index_to_word_dict, idf_headline_test)\n",
    "\n",
    "#-----------------word2vec-feature1-------------------------\n",
    "data_method2_train = body_headline_match(word2vec_1_body_train, word2vec_1_headline_train, train_bodies, train_stances)\n",
    "data_method2_test = body_headline_match(word2vec_1_body_test, word2vec_1_headline_test, test_bodies, test_stances)\n",
    "# feature_word2vec_1_cosine_similarity = data_method2.distance_or_similarity.values\n",
    "\n",
    "print('word2vec feature1 finished')\n",
    "\n",
    "#-----------------word2vec-representation2-------------------------\n",
    "#extract n most important words from body and headline according to the tf-idf value\n",
    "important_words_body_train = tf_idf_based_search(tf_idf_body_train, n=5)\n",
    "important_words_headline_train = tf_idf_based_search(tf_idf_headline_train, n=5)\n",
    "important_words_body_test = tf_idf_based_search(tf_idf_body_test, n=5)\n",
    "important_words_headline_test = tf_idf_based_search(tf_idf_headline_test, n=5)\n",
    "\n",
    "\n",
    "#-----------------word2vec-feature2-------------------------\n",
    "data_method3_train = word_mover_distance(important_words_body_train, \n",
    "                                         important_words_headline_train, \n",
    "                                         train_bodies, \n",
    "                                         train_stances, \n",
    "                                         index_to_word_dict, \n",
    "                                         n=5)\n",
    "\n",
    "data_method3_test = word_mover_distance(important_words_body_test, \n",
    "                                        important_words_headline_test, \n",
    "                                        test_bodies, \n",
    "                                        test_stances, \n",
    "                                        index_to_word_dict, \n",
    "                                        n=5)\n",
    "\n",
    "print('word2vec feature2 finished')\n",
    "\n",
    "#-----------------doc2vec--------------------------\n",
    "\n",
    "train_body['articleBody'] = transform(index_to_word_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(index_to_word_dict, train_stance.Headline)\n",
    "test_body['articleBody'] = transform(index_to_word_dict, test_body.articleBody)\n",
    "test_stance['Headline'] = transform(index_to_word_dict, test_stance.Headline)\n",
    "#doc2vec representation and cosine similarity feature\n",
    "#get document of the doc2vec model\n",
    "document = get_doc2vec_document(train_body, train_stance, test_body, test_stance)\n",
    "\n",
    "#different parameters lead to different performance\n",
    "\n",
    "#define the doc2vecm model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=128, alpha=.025, min_alpha=.01, dm=0, worker=8, dbow_words=1)\n",
    "model.build_vocab(document)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(20):\n",
    "    model.train(document, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002  # decrease the learning rate`\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "#training data\n",
    "#construct Dataframe for body and headline to merge the data on Body ID\n",
    "body_train = pd.DataFrame(list(range(train_body.shape[0])), columns=['body_idx'])\n",
    "body_train['Body ID'] = train_bodies['Body ID']\n",
    "headline_train = pd.DataFrame(list(range(train_stance.shape[0])), columns=['headline_idx'])\n",
    "headline_train['Body ID'] = train_stances['Body ID']\n",
    "headline_train['Stance'] = train_stances['Stance']\n",
    "\n",
    "#merge the body and the headline\n",
    "data_method4_train = body_stance_join(body_train, headline_train)\n",
    "\n",
    "#get body index and headline index from the vector matrix\n",
    "body_idx = data_method4_train['body_idx'].tolist()\n",
    "headline_idx = data_method4_train['headline_idx'].tolist()\n",
    "\n",
    "similarity = np.zeros(data_method4_train.shape[0])\n",
    "\n",
    "for i in range(data_method4_train.shape[0]):\n",
    "    similarity[i] = model.docvecs.similarity(d1='train_body_'+str(body_idx[i]), \n",
    "                                             d2='train_headline_'+str(headline_idx[i]))\n",
    "data_method4_train['distance_or_similarity'] = similarity\n",
    "\n",
    "\n",
    "#test set\n",
    "body_test = pd.DataFrame(list(range(test_body.shape[0])), columns=['body_idx'])\n",
    "body_test['Body ID'] = test_bodies['Body ID']\n",
    "headline_test = pd.DataFrame(list(range(test_stance.shape[0])), columns=['headline_idx'])\n",
    "headline_test['Body ID'] = test_stances['Body ID']\n",
    "headline_test['Stance'] = test_stances['Stance']\n",
    "\n",
    "#merge the body and the headline\n",
    "data_method4_test = body_stance_join(body_test, headline_test)\n",
    "\n",
    "#get body index and headline index from the vector matrix\n",
    "body_idx = data_method4_test['body_idx'].tolist()\n",
    "headline_idx = data_method4_test['headline_idx'].tolist()\n",
    "\n",
    "similarity = np.zeros(data_method4_test.shape[0])\n",
    "\n",
    "for i in range(data_method4_test.shape[0]):\n",
    "    similarity[i] = model.docvecs.similarity(d1='test_body_'+str(body_idx[i]), \n",
    "                                             d2='test_headline_'+str(headline_idx[i]))\n",
    "data_method4_test['distance_or_similarity'] = similarity\n",
    "\n",
    "\n",
    "\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "test_body['articleBody'] = transform(word_to_index_dict, test_body.articleBody)\n",
    "test_stance['Headline'] = transform(word_to_index_dict, test_stance.Headline)\n",
    "\n",
    "print('doc2vec finished')\n",
    "\n",
    "\n",
    "# #-----------------language-model-------------------------\n",
    "# #language model representation and KL divergance feature\n",
    "\n",
    "# #get the background of body and headline\n",
    "# #As the corpus is obtained by combining the body and stance, in background_body, there are some wrods that not appear\n",
    "# #in the body article and their probability will be zeros\n",
    "# background_body_train = get_background(train_body['articleBody'], tf_body_train) + 1e-05\n",
    "# background_headline_train = get_background(train_stance['Headline'], tf_headline_train) + 1e-05\n",
    "# background_body_test = get_background(test_body['articleBody'], tf_body_test) + 1e-05\n",
    "# background_headline_test = get_background(test_stance['Headline'], tf_headline_test) + 1e-05\n",
    "\n",
    "# #get the language model of each individual body and headline\n",
    "# language_model_body_train = get_language_model(train_body['articleBody'], background_body_train, tf_body_train, smoothing='dirchelet')\n",
    "# language_model_headline_train = get_language_model(train_stance['Headline'], background_headline_train, tf_headline_train, smoothing='dirchelet')\n",
    "# language_model_body_test = get_language_model(test_body['articleBody'], background_body_test, tf_body_test, smoothing='dirchelet')\n",
    "# language_model_headline_test = get_language_model(test_stance['Headline'], background_headline_test, tf_headline_test, smoothing='dirchelet')\n",
    "\n",
    "# #get the pair-wise KL divergance(might takes about 5 minutes to compute)\n",
    "# data_KL_train = body_headline_match(language_model_body_train, language_model_headline_train, train_bodies, train_stances, distance='KL')\n",
    "# data_KL_test = body_headline_match(language_model_body_test, language_model_headline_test, test_bodies, test_stances, distance='KL')\n",
    "\n",
    "# print('language model finished')\n",
    "\n",
    "\n",
    "#-----------------LDA-model-------------------------\n",
    "\n",
    "#-----------------different-preprocess-------------------------\n",
    "merged_bodies, merged_stances = preprocess(train_bodies, train_stances, test_bodies, test_stances, lemma=False)\n",
    "\n",
    "#split the train the test set so that we can construct feature from the training set\n",
    "train_body, test_body = train_test_split(merged_bodies, train_bodies, test_bodies, dtype='body')\n",
    "train_stance, test_stance = train_test_split(merged_stances, train_stances, test_stances, dtype='stance')\n",
    "\n",
    "# get the corpus(mapping between index and word) for all the data set\n",
    "corpus, word_to_index_dict, index_to_word_dict = get_corpus(merged_bodies, merged_stances)\n",
    "\n",
    "#transform the document from string to integer\n",
    "train_body['articleBody'] = transform(word_to_index_dict, train_body.articleBody)\n",
    "train_stance['Headline'] = transform(word_to_index_dict, train_stance.Headline)\n",
    "\n",
    "test_body['articleBody'] = transform(word_to_index_dict, test_body.articleBody)\n",
    "test_stance['Headline'] = transform(word_to_index_dict, test_stance.Headline)\n",
    "\n",
    "#get tf matrix, idf matrix and tf_idf matrix for body and headline\n",
    "tf_body_train, idf_body_train, tf_idf_body_train = get_tf_idf_matrix(corpus, train_body.articleBody, count=True)\n",
    "tf_headline_train, idf_headline_train, tf_idf_headline_train = get_tf_idf_matrix(corpus, train_stance.Headline, count=True)\n",
    "\n",
    "tf_body_test, idf_body_test, tf_idf_body_test = get_tf_idf_matrix(corpus, test_body.articleBody, count=True)\n",
    "tf_headline_test, idf_headline_test, tf_idf_headline_test = get_tf_idf_matrix(corpus, test_stance.Headline, count=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#topic model LDA feature\n",
    "lda = LatentDirichletAllocation(n_topics=25, learning_method='batch', n_jobs=3, random_state=0)\n",
    "lda_body_train = lda.fit_transform(tf_body_train)\n",
    "lda_body_test = lda.transform(tf_body_test)\n",
    "lda_headline_train = lda.transform(tf_headline_train)\n",
    "lda_headline_test = lda.transform(tf_headline_test)\n",
    "\n",
    "data_lda_train = body_headline_match(vector_body=lda_body_train, vector_headline=lda_headline_train, bodies=train_bodies, stances=train_stances, distance='cos')\n",
    "data_lda_test = body_headline_match(vector_body=lda_body_test, vector_headline=lda_headline_test, bodies=test_bodies, stances=test_stances, distance='cos')\n",
    "\n",
    "print('LDA finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:55:51.551464Z",
     "start_time": "2018-03-31T20:55:51.519522Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def data_match(body, headline, bodies, stances):\n",
    "    data_body = pd.DataFrame(body)\n",
    "    data_headline = pd.DataFrame(headline)\n",
    "    \n",
    "    data_body['Body ID'] = bodies['Body ID']\n",
    "    data_headline['Body ID'] = stances['Body ID']\n",
    "    data_headline['Stance'] = stances['Stance']\n",
    "    data_headline['headline_idx'] = data_headline.index\n",
    "    \n",
    "    merged = body_stance_join(data_body, data_headline)\n",
    "    merged = merged.drop(['Body ID'], axis=1)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:55:51.656433Z",
     "start_time": "2018-03-31T20:55:51.555482Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def feature_match(data, feature):\n",
    "    feature = feature.drop(['body_idx', 'Body ID', 'Stance'], axis=1)\n",
    "    merged = pd.merge(data, feature, on='headline_idx')\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T20:55:51.685775Z",
     "start_time": "2018-03-31T20:55:51.662266Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#select n most frequent word(highest average tf-idf value)and use it as the vector representation\n",
    "def select_n_words(vec_body, vec_headline, n=5000):\n",
    "    data = np.vstack([vec_body, vec_headline])\n",
    "    data = np.sum(data, axis=0)\n",
    "    idx_list = data.argsort()[::-1][:n]\n",
    "    \n",
    "    vec_body = vec_body.T\n",
    "    vec_headline = vec_headline.T\n",
    "    \n",
    "    vec_body = vec_body[idx_list]\n",
    "    vec_headline = vec_headline[idx_list]\n",
    "    \n",
    "    return vec_body.T, vec_headline.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T16:28:30.149067Z",
     "start_time": "2018-04-01T16:28:30.134190Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def x_y_split(data):\n",
    "    x = data.drop(['Stance'], axis=1)\n",
    "    y = data.Stance\n",
    "    \n",
    "    y_agree = y.map(lambda x: x == 'agree').values.reshape(-1,1)\n",
    "    y_disagree = y.map(lambda x: x == 'disagree').values.reshape(-1,1)\n",
    "    y_discuss = y.map(lambda x: x == 'discuss').values.reshape(-1,1)\n",
    "    y_unrelated = y.map(lambda x: x == 'unrelated').values.reshape(-1,1)\n",
    "    \n",
    "    y = np.hstack([y_agree, y_disagree, y_discuss, y_unrelated])\n",
    "    \n",
    "    return x.values, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T21:09:55.313102Z",
     "start_time": "2018-03-31T20:55:51.688892Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BOW_body_train = tf_idf_body_train\n",
    "BOW_headline_train = tf_idf_headline_train\n",
    "\n",
    "BOW_body_test = tf_idf_body_test\n",
    "BOW_headline_test = tf_idf_headline_test\n",
    "\n",
    "BOW_body_train, BOW_headline_train = select_n_words(BOW_body_train, BOW_headline_train, 5000)\n",
    "BOW_body_test, BOW_headline_test = select_n_words(BOW_body_test, BOW_headline_test, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T21:12:55.480103Z",
     "start_time": "2018-03-31T21:09:55.352602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vector_body_train = lda_body_train\n",
    "vector_headline_train = lda_headline_train\n",
    "\n",
    "vector_body_test = lda_body_test\n",
    "vector_headline_test = lda_headline_train\n",
    "\n",
    "\n",
    "data_train = data_match(vector_body_train, vector_headline_train, train_bodies, train_stances)\n",
    "data_train = feature_match(data_train, data_method1_train)\n",
    "data_train = feature_match(data_train, data_method2_train)\n",
    "data_train = feature_match(data_train, data_method3_train)\n",
    "data_train = feature_match(data_train, data_method4_train)\n",
    "data_train = feature_match(data_train, data_lda_train)\n",
    "\n",
    "\n",
    "data_test = a = data_match(vector_body_test, vector_headline_test, test_bodies, test_stances)\n",
    "data_test = feature_match(data_test, data_method1_test)\n",
    "data_test = feature_match(data_test, data_method1_test)\n",
    "data_test = feature_match(data_test, data_method1_test)\n",
    "data_test = feature_match(data_test, data_method1_test)\n",
    "data_test = feature_match(data_test, data_lda_test)\n",
    "\n",
    "\n",
    "#drop the headline_idx columns\n",
    "data_train = data_train.drop(['headline_idx'], axis=1)\n",
    "data_test = data_test.drop(['headline_idx'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:45:43.528920Z",
     "start_time": "2018-04-01T10:45:43.524104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data_train = pd.read_csv('lda_train.csv', index_col=0)\n",
    "# data_test = pd.read_csv('lda_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T11:45:31.706484Z",
     "start_time": "2018-04-01T11:45:31.174795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val = train_val_split(data_train)\n",
    "\n",
    "x_train, y_train = x_y_split(train)\n",
    "x_val, y_val = x_y_split(val)\n",
    "x_test, y_test = x_y_split(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T16:39:16.942084Z",
     "start_time": "2018-04-01T16:39:16.929160Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \n",
    "    m_sample = y_true.shape[0]\n",
    "    n_class = y_true.shape[1]\n",
    "    \n",
    "    temp = (y_pred == y_true).sum(axis=1)\n",
    "    acc = (temp == n_class).sum() / m_sample\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T16:39:17.262632Z",
     "start_time": "2018-04-01T16:39:17.211573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true):\n",
    "    def change(y):\n",
    "        m = y.shape[0]\n",
    "        y_label = []\n",
    "        for i in range(m):\n",
    "            idx = y[i].argmax()\n",
    "            if idx == 0:\n",
    "                y_label.append('agree')\n",
    "            elif idx == 1:\n",
    "                y_label.append('disagree') \n",
    "            elif idx == 2:\n",
    "                y_label.append('discuss')\n",
    "            elif idx == 3:\n",
    "                y_label.append('unrelated')\n",
    "\n",
    "        return y_label\n",
    "    \n",
    "    y_pred_label = change(y_pred)\n",
    "    y_true_label = change(y_true)\n",
    "    \n",
    "    cfm = CFM(y_pred=y_pred_label, y_true=y_true_label)\n",
    "    \n",
    "    return cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Regression train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:39:48.484561Z",
     "start_time": "2018-04-01T10:39:48.463974Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_linear(x, y, x_val):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x,y)\n",
    "    return model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T16:38:49.770701Z",
     "start_time": "2018-04-01T16:38:49.759155Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label_predict(y):\n",
    "    y_pred = np.zeros_like(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        y_pred[i][y[i].argmax()] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:51:08.969007Z",
     "start_time": "2018-04-01T10:51:08.434469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get prediction of each stance class\n",
    "y_agree = fit_predict_linear(x_train, y_train[:,0], x_test).reshape(-1,1)\n",
    "y_disagree = fit_predict_linear(x_train, y_train[:,1], x_test).reshape(-1,1)\n",
    "y_discuss = fit_predict_linear(x_train, y_train[:,2], x_test).reshape(-1,1)\n",
    "y_unrelated = fit_predict_linear(x_train, y_train[:,3], x_test).reshape(-1,1)\n",
    "\n",
    "#stack the prediction\n",
    "y_pred = np.hstack([y_agree1, y_disagree, y_discuss, y_unrelated])\n",
    "\n",
    "#change the prediction to predicted label\n",
    "y_pred = label_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:39:49.664085Z",
     "start_time": "2018-04-01T10:39:49.529485Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get confusion matrix and accuracy\n",
    "cfm_linear = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "accuracy_linear = accuracy(y_pred, y_test)\n",
    "auc_linear = sklearn.metrics.roc_auc_score(y_score=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic Regression train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:40:25.055482Z",
     "start_time": "2018-04-01T10:40:25.011738Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        exp_minmax = lambda x: np.exp(x - np.max(x))\n",
    "        denom = lambda x: 1.0 / np.sum(x)\n",
    "        x = np.apply_along_axis(exp_minmax, 1, x)\n",
    "        denominator = np.apply_along_axis(denom, 1, x)\n",
    "\n",
    "        if len(denominator.shape) == 1:\n",
    "            denominator = denominator.reshape((denominator.shape[0], 1))\n",
    "\n",
    "        x = x * denominator\n",
    "    else:\n",
    "        # Vector\n",
    "        x_max = np.max(x)\n",
    "        x = x - x_max\n",
    "        numerator = np.exp(x)\n",
    "        denominator = 1.0 / np.sum(numerator)\n",
    "        x = numerator.dot(denominator)\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T10:40:25.846212Z",
     "start_time": "2018-04-01T10:40:25.838267Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_logistic(x, y, x_val):\n",
    "    model = LogisticRegression(C=0.1)\n",
    "    model.fit(x,y)\n",
    "    return model.predict_proba(x_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T14:34:57.540607Z",
     "start_time": "2018-04-01T14:34:55.160352Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get prediction of each stance\n",
    "y_agree = predict_label(x_train, y_train[:,0], x_test).reshape(-1,1)\n",
    "y_disagree = predict_label(x_train, y_train[:,1], x_test).reshape(-1,1)\n",
    "y_discuss = predict_label(x_train, y_train[:,2], x_test).reshape(-1,1)\n",
    "y_unrelated = predict_label(x_train, y_train[:,3], x_test).reshape(-1,1)\n",
    "\n",
    "#stack the prediction\n",
    "y_pred = np.hstack([y_agree, y_disagree, y_discuss, y_unrelated])\n",
    "\n",
    "#apply softmax funcition\n",
    "y_pred = softmax(y_pred)\n",
    "\n",
    "#get the predicted label\n",
    "y_pred = label_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T14:34:58.392967Z",
     "start_time": "2018-04-01T14:34:58.201628Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get the final result\n",
    "cfm_logstic = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "accuracy_logistic = accuracy(y_pred, y_test)\n",
    "auc_logistic = sklearn.metrics.roc_auc_score(y_score=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T19:41:47.225515Z",
     "start_time": "2018-04-01T19:41:47.219642Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T17:13:00.229161Z",
     "start_time": "2018-04-01T17:13:00.224293Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]\n",
    "n_class = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T22:30:51.700249Z",
     "start_time": "2018-04-01T22:30:35.640842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49972 samples, validate on 25413 samples\n",
      "Epoch 1/6\n",
      "49972/49972 [==============================] - 6s 129us/step - loss: 0.5440 - val_loss: 1.5908\n",
      "Epoch 2/6\n",
      "49972/49972 [==============================] - 2s 37us/step - loss: 0.3224 - val_loss: 0.9811\n",
      "Epoch 3/6\n",
      "49972/49972 [==============================] - 2s 37us/step - loss: 0.2973 - val_loss: 0.5580\n",
      "Epoch 4/6\n",
      "49972/49972 [==============================] - 2s 36us/step - loss: 0.2875 - val_loss: 0.3920\n",
      "Epoch 5/6\n",
      "49972/49972 [==============================] - 2s 32us/step - loss: 0.2786 - val_loss: 0.3582\n",
      "Epoch 6/6\n",
      "49972/49972 [==============================] - 2s 33us/step - loss: 0.2742 - val_loss: 0.3431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x162decf60>"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras MLP mode\n",
    "model = Sequential()\n",
    "\n",
    "#layers\n",
    "model.add(Dense(128, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#complie\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Nadam')\n",
    "\n",
    "#fit model\n",
    "model.fit(x_train, y_train, nb_epoch=6, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T23:00:51.083988Z",
     "start_time": "2018-04-01T23:00:49.815485Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_mlp = model.predict(x_test)\n",
    "y_pred = label_predict(pred_mlp)\n",
    "\n",
    "#get confusion matrix and accuracy\n",
    "cfm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "auc = sklearn.metrics.roc_auc_score(y_score=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T23:00:51.613231Z",
     "start_time": "2018-04-01T23:00:51.606857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8697517018848621"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuract\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T23:00:52.148062Z",
     "start_time": "2018-04-01T23:00:52.142127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  141,     0,  1626,   136],\n",
       "       [   71,     0,   490,   136],\n",
       "       [  189,     0,  3980,   295],\n",
       "       [   20,     0,   347, 17982]])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "#1:Agree\n",
    "#2:disagree\n",
    "#3:discuss\n",
    "#4:unrelated\n",
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
