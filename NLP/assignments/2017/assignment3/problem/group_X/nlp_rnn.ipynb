{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = data_train[40000:]\n",
    "# len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWord2Vec(filename):\n",
    "    vocab = {}\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        for i,token in enumerate(row):\n",
    "            if i == 0:\n",
    "                vocab[row[i]] = len(vocab)\n",
    "            else:\n",
    "                embd.append(float(row[i]))\n",
    "    print('Successfully load.')\n",
    "    file.close()    \n",
    "    return vocab,embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(input):\n",
    "    token = re.compile(\"[\\w]+(?=n't)|n't|'s|\\'m|\\'ll|[\\w]+|[.?!;,:]\")\n",
    "    tokens = token.findall(input)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline2(data,vocab=None,embd=None,max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None and embd is None: \n",
    "        is_ext_vocab = False\n",
    "        #load word2vect\n",
    "        filename = 'word2vec.txt'\n",
    "        vocab, vec = loadWord2Vec(filename)\n",
    "        embd = np.array(vec,dtype=np.float32)\n",
    "        embd = np.reshape(embd,(len(vocab),50))\n",
    "    \n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                token = token.lower()\n",
    "                if not is_ext_vocab: #trainning set\n",
    "                    token_id = vocab[token]\n",
    "                elif token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]                 \n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        data_sentences.append(sents)\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_orders, vocab, embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load.\n"
     ]
    }
   ],
   "source": [
    "train_stories, train_orders, vocab, embd = pipeline2(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45502, 5, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = train_stories.shape[2]\n",
    "dev_stories, dev_orders, _, _ = pipeline2(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories = np.reshape(train_stories,(-1,20))\n",
    "train_orders = np.reshape(train_orders,(-1,1))\n",
    "dev_stories = np.reshape(dev_stories,(-1,20))\n",
    "dev_orders = np.reshape(dev_orders,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227510, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.convert_to_tensor(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(26516, 50) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameter\n",
    "num_hidden = 128\n",
    "target_size = 5\n",
    "vocab_size = len(vocab)\n",
    "input_size = 50\n",
    "timesteps = 5\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#creat tensor holder\n",
    "story = tf.placeholder(tf.int32, [None,None,None],\"story\")\n",
    "order = tf.placeholder(tf.int32, [None,None],\"order\")\n",
    "embedding = tf.placeholder(tf.float32,[vocab_size,input_size],\"embed\")\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, target_size]),dtype=tf.float32)\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([target_size]),dtype=tf.float32)\n",
    "}\n",
    "\n",
    "#embedding word\n",
    "batch_size = tf.shape(story)[0]\n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(axis=1, num_or_size_splits=5, value=story)]  # 5 times [batch_size x max_length]\n",
    "# initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "# embeddings = tf.get_variable(\"W\", [vocab_size, input_size], initializer=initializer)\n",
    "embeddings = tf.Variable(tf.constant(0.0,shape = [vocab_size,input_size]),trainable = False)#false means that we don't update the word vector\n",
    "embedding_init = embeddings.assign(embedding)\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # [batch_size x max_seq_length x input_size]\n",
    "                    for sentence in sentences]\n",
    "hs = [tf.reduce_sum(sentence, 1) for sentence in sentences_embedded] # 5 times [batch_size x input_size]\n",
    "# hs = tf.reshape(hs,[5,batch_size,input_size])\n",
    "\n",
    "#build multiRNNCell\n",
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "#     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    cells.append(cell)\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "output, states = rnn.static_rnn(cell,hs,dtype=tf.float32)\n",
    "\n",
    "#basiclstmcell\n",
    "# lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "# outputs, states = rnn.static_rnn(lstm_cell,hs,dtype=tf.float32)\n",
    "\n",
    "\n",
    "logits = []\n",
    "for eachoutput in output:\n",
    "    logits.append(tf.matmul(eachoutput, weights['out']) + biases['out'])\n",
    "logits = tf.concat(axis=1, values=logits)#[batch_size x 5*input_size]\n",
    "logits = tf.reshape(logits, [-1, 5, target_size]) #[batch,5,target]\n",
    "\n",
    "# calculate loss \n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))\n",
    "\n",
    "# prediction function\n",
    "unpacked_logits = [tensor for tensor in tf.unstack(logits, axis=1)]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "softmaxed_logits = tf.stack(softmaxes, axis=1)\n",
    "predict = tf.arg_max(softmaxed_logits, 2)\n",
    "opt_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 -----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (25, 20) for Tensor 'story:0', which has shape '(?, ?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7e82cadb19df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0minst_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_orders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Train loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1101\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (25, 20) for Tensor 'story:0', which has shape '(?, ?, ?)'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 25\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    n = train_stories.shape[0]                \n",
    "    for epoch in range(10000):\n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        total_loss = 0\n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            inst_story = train_stories[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "            inst_order = train_orders[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "            feed_dict = {story: inst_story, order: inst_order,embedding : embd}\n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss += current_loss\n",
    "        print(' Train loss:', total_loss / n)\n",
    "        \n",
    "\n",
    "        train_feed_dict = {story: train_stories, order: train_orders,embedding:embd}\n",
    "        train_predicted = sess.run(predict, feed_dict=train_feed_dict)\n",
    "        train_accuracy = nn.calculate_accuracy(train_orders, train_predicted)\n",
    "        print(' Train accuracy:', train_accuracy)\n",
    "        \n",
    "        dev_feed_dict = {story: dev_stories, order: dev_orders,embedding: embd}\n",
    "        dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "        print(' Dev accuracy:', dev_accuracy)\n",
    "\n",
    "        \n",
    "    \n",
    "    nn.save_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
